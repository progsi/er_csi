{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rapidfuzz import process, fuzz, utils\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "shs100k = pd.read_parquet(\"../data/shs100k2_yt.parquet\")\n",
    "datacos = pd.read_parquet(\"../data/datacos_yt.parquet\")\n",
    "shs_yt = pd.read_parquet(\"../data/shs_yt_yt.parquet\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rapid Fuzz Random Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "song title: spanish is the loving tongue\n",
      "video title: crew cuts - spanish is a loving tongue\n",
      "video description: this was released on vee jay 569 in 1963, it's the flip of the three bells.  this was their only vee jay release, i'm not aware of any vee jay lp. i have never seen a stock copy of this record, but have heard it played on the radio a few times.  i hope you like it, feel free to comment.\n",
      "st-vt 92.3076923076923\n",
      "vt-st 92.3076923076923\n",
      "vd-st 19.354838709677423\n",
      "st-vd 19.354838709677423\n"
     ]
    }
   ],
   "source": [
    "random_video = shs100k.sample(n=1)\n",
    "random_video.head()\n",
    "\n",
    "song_title = random_video.title.item().lower()\n",
    "video_title = random_video.video_title.item().lower()\n",
    "video_description = random_video.description.item().lower()\n",
    "\n",
    "# song title in video title\n",
    "st_vt = fuzz.token_set_ratio(song_title, video_title)\n",
    "vt_st = fuzz.token_set_ratio(video_title, song_title)\n",
    "# song title in description\n",
    "st_vd = fuzz.token_set_ratio(song_title, video_description)\n",
    "vd_st = fuzz.token_set_ratio(video_description, song_title)\n",
    "\n",
    "print(f\"song title: {song_title}\")\n",
    "print(f\"video title: {video_title}\")\n",
    "print(f\"video description: {video_description}\")\n",
    "\n",
    "print(f\"st-vt {st_vt}\")\n",
    "print(f\"vt-st {vt_st}\")\n",
    "print(f\"vd-st {st_vd}\")\n",
    "print(f\"st-vd {vd_st}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ChatGPT tries...gen NER dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'This': 0, 'is': 0, 'a': 0, 'longer': 1, 'sequence': 1, 'with': 0, 'more': 0, 'tokens': 0, '.': 0}\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "def create_ner_dataset(longer_sequence, shorter_sequence):\n",
    "    # Load spaCy English model for tokenization\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "    # Tokenize the longer and shorter sequences\n",
    "    longer_tokens = [token.text for token in nlp(longer_sequence)]\n",
    "    shorter_tokens = [token.text for token in nlp(shorter_sequence)]\n",
    "\n",
    "    # Initialize the output dictionary with all tokens set to 0\n",
    "    output_dict = {token: 0 for token in longer_tokens}\n",
    "\n",
    "    # Find the start index of the overlapping region\n",
    "    start_index = longer_tokens.index(shorter_tokens[0])\n",
    "\n",
    "    # Mark the tokens in the overlapping region as 1\n",
    "    for i in range(start_index, start_index + len(shorter_tokens)):\n",
    "        output_dict[longer_tokens[i]] = 1\n",
    "\n",
    "    return output_dict\n",
    "\n",
    "# Example usage\n",
    "longer_sequence = \"This is a longer sequence with more tokens.\"\n",
    "shorter_sequence = \"longer sequence\"\n",
    "result = create_ner_dataset(longer_sequence, shorter_sequence)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 'This', 'UNKN'), (1, 'is', 'UNKN'), (2, 'a', 'UNKN'), (3, 'longer', 'TITLE'), (4, 'sequence', 'TITLE'), (5, 'with', 'UNKN'), (6, 'more', 'PERFORMER'), (7, 'tokens', 'PERFORMER'), (8, '.', 'UNKN')], [(0, 'Another', 'TITLE'), (1, 'video', 'UNKN'), (2, 'title', 'TITLE')]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "\n",
    "def create_ner_dataset(row):\n",
    "    # Tokenize the longer and shorter sequences\n",
    "    longer_tokens = [(token.text, 'UNKN') for token in nlp(row['video_title'])]\n",
    "    title_tokens = [(token.text, 'TITLE') for token in nlp(row['title'])]\n",
    "    performer_tokens = [(token.text, 'PERFORMER') for token in nlp(row['performer'])]\n",
    "\n",
    "    # Create sets of tokens for 'title' and 'performer'\n",
    "    title_set = set(token for token, _ in title_tokens)\n",
    "    performer_set = set(token for token, _ in performer_tokens)\n",
    "\n",
    "    # Initialize the output list of tuples\n",
    "    output_list = []\n",
    "\n",
    "    # Populate the output list with tuples from 'title' and 'performer'\n",
    "    for i, (token, _) in enumerate(longer_tokens):\n",
    "        if token in title_set:\n",
    "            output_list.append((i, token, 'TITLE'))\n",
    "        elif token in performer_set:\n",
    "            output_list.append((i, token, 'PERFORMER'))\n",
    "        else:\n",
    "            output_list.append((i, token, 'UNKN'))\n",
    "\n",
    "    return output_list\n",
    "\n",
    "# Example DataFrame\n",
    "data = {'video_title': [\"This is a longer sequence with more tokens.\", \"Another video title\"],\n",
    "        'title': [\"longer sequence\", \"Another title\"],\n",
    "        'performer': [\"more tokens\", \"Another performer\"]}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Load spaCy English model for tokenization\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Apply the create_ner_dataset function to each row of the DataFrame\n",
    "result_list = df.apply(create_ner_dataset, axis=1).tolist()\n",
    "\n",
    "# Print the list of lists of tuples\n",
    "print(result_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'shs100k' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 21\u001b[0m\n\u001b[1;32m     16\u001b[0m     seq_qry \u001b[38;5;241m=\u001b[39m encode(qry, doc)\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m seq_qry\n\u001b[0;32m---> 21\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mshs100k\u001b[49m\u001b[38;5;241m.\u001b[39mhead()\n\u001b[1;32m     22\u001b[0m df[[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvideo_title\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mperformer\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'shs100k' is not defined"
     ]
    }
   ],
   "source": [
    "def ner_row(row, col=\"title\"):\n",
    "    \n",
    "    def encode(sub, sent):\n",
    "        subwords, sentwords = sub.lower().split(), sent.lower().split()\n",
    "        print(subwords)\n",
    "        res = [\"UNK\" for _ in sentwords]    \n",
    "        for i, word in enumerate(sentwords[:-len(subwords) + 1]):\n",
    "            if all(x == y for x, y in zip(subwords, sentwords[i:i + len(subwords)])):\n",
    "                for j in range(len(subwords)):\n",
    "                    res[i + j] = col.upper()\n",
    "        return res\n",
    "    \n",
    "    doc = row.video_title\n",
    "    qry = row[col]\n",
    "    \n",
    "    seq_qry = encode(qry, doc)\n",
    "\n",
    "    return seq_qry\n",
    "\n",
    "\n",
    "df = shs100k.head()\n",
    "df[[\"video_title\", \"title\", \"performer\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['yesterday']\n",
      "['yesterday']\n",
      "['yesterday']\n",
      "['yesterday']\n",
      "['yesterday']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    [UNK, UNK, UNK, UNK, UNK, UNK]\n",
       "1              [UNK, UNK, UNK, UNK]\n",
       "4              [UNK, UNK, UNK, UNK]\n",
       "5              [UNK, UNK, UNK, UNK]\n",
       "6              [UNK, UNK, UNK, UNK]\n",
       "dtype: object"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.apply(lambda x: ner_row(x, 'title'), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def encode(sub, sent):\n",
    "    subwords, sentwords = sub.split(), sent.split()\n",
    "    res = [0 for _ in sentwords]    \n",
    "    for i, word in enumerate(sentwords[:-len(subwords) + 1]):\n",
    "        if all(x == y for x, y in zip(subwords, sentwords[i:i + len(subwords)])):\n",
    "            for j in range(len(subwords)):\n",
    "                res[i + j] = 1\n",
    "    return res\n",
    "\n",
    "def encode2(sub, sent):\n",
    "    subword, sentwords = sub, sent.split()\n",
    "    res = [0] * len(sentwords)\n",
    "\n",
    "    for i, word in enumerate(sentwords):\n",
    "        if word == subword:\n",
    "            res[i] = 1\n",
    "\n",
    "    return res\n",
    "\n",
    "encode2(\"yesterday i\", \"marianne faithful yesterday i\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "81.81818181818181"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q=\"You're All I Need to Get By\"\n",
    "d=\"DIANA ROSS  you're all i need to get by\"\n",
    "\n",
    "fuzz.QRatio(q.lower(), d.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "NER = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "doc = NER(d)\n",
    "entities = [(e.label_,e.text) for e in doc.ents]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('PERSON', 'DIANA ROSS')]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "qry col: title\n",
      "doc col: video_title\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/11 [00:22<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 18\u001b[0m\n\u001b[1;32m     14\u001b[0m match_dict[query_column][document_column] \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dist \u001b[38;5;129;01min\u001b[39;00m tqdm(fuzz\u001b[38;5;241m.\u001b[39m__all__):\n\u001b[0;32m---> 18\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcdist\u001b[49m\u001b[43m(\u001b[49m\u001b[43mqueries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshs100k\u001b[49m\u001b[43m[\u001b[49m\u001b[43mquery_column\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_list\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mchoices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshs100k\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdocument_column\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_list\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m     match_dict[query_column][document_column][dist] \u001b[38;5;241m=\u001b[39m results\n",
      "File \u001b[0;32m~/miniconda3/envs/torch2/lib/python3.10/site-packages/rapidfuzz/process_cpp.py:75\u001b[0m, in \u001b[0;36mcdist\u001b[0;34m(queries, choices, scorer, processor, score_cutoff, score_hint, score_multiplier, dtype, workers, **kwargs)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m     73\u001b[0m dtype \u001b[38;5;241m=\u001b[39m _dtype_to_type_num(dtype)\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39masarray(\n\u001b[0;32m---> 75\u001b[0m     \u001b[43m_cdist\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[43m        \u001b[49m\u001b[43mqueries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchoices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscorer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscorer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprocessor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprocessor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscore_cutoff\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscore_cutoff\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscore_hint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscore_hint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscore_multiplier\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscore_multiplier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[43m        \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     87\u001b[0m )\n",
      "File \u001b[0;32msrc/rapidfuzz/process_cpp_impl.pyx:1568\u001b[0m, in \u001b[0;36mrapidfuzz.process_cpp_impl.cdist\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32msrc/rapidfuzz/process_cpp_impl.pyx:1449\u001b[0m, in \u001b[0;36mrapidfuzz.process_cpp_impl.cdist_two_lists\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# query (title, performer)\n",
    "# yt attr (video title, channel name, description)\n",
    "match_dict = {}\n",
    "\n",
    "for query_column in [\"title\", \"performer\"]:\n",
    "    \n",
    "    print(f\"qry col: {query_column}\")\n",
    "    match_dict[query_column] = {}\n",
    "    \n",
    "    for document_column in tqdm([\"video_title\", \"description\", \"channel_name\"]):\n",
    "        \n",
    "        print(f\"doc col: {document_column}\")\n",
    "\n",
    "        match_dict[query_column][document_column] = {}\n",
    "        \n",
    "                        \n",
    "        results = process.cdist(queries=shs100k[query_column].to_list(), \n",
    "                          choices=shs100k[document_column].to_list(), )\n",
    "                        \n",
    "        match_dict[query_column][document_column][\"token_set_ratio\"] = results\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>video_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>30125</th>\n",
       "      <td>Manha de Carnaval</td>\n",
       "      <td>Mason Williams</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   title     video_title\n",
       "30125  Manha de Carnaval  Mason Williams"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shs100k2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
