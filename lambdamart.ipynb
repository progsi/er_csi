{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import scipy\n",
    "from torchmetrics.retrieval import RetrievalNormalizedDCG, RetrievalMAP\n",
    "from src.dataset import TestDataset, OnlineCoverSongDataset\n",
    "from src.evaluation import RetrievalEvaluation\n",
    "from src.baselines.blocking import Blocker\n",
    "from rapidfuzz import fuzz\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "mean_average_precision = RetrievalMAP(empty_target_action=\"skip\")\n",
    "\n",
    "def mean_rank_1(preds, target):\n",
    "        \"\"\"\n",
    "        Compute the mean rank for relevant items in the predictions.\n",
    "        Args:\n",
    "            preds (torch.Tensor): A tensor of predicted scores (higher scores indicate more relevant items).\n",
    "            target (torch.Tensor): A tensor of true relationships (0 for irrelevant, 1 for relevant).\n",
    "        Returns:\n",
    "            float: The mean rank of relevant items for each query.\n",
    "        \"\"\"\n",
    "        has_positives = torch.sum(target, 1) > 0\n",
    "        \n",
    "        _, spred = torch.topk(preds, preds.size(1), dim=1)\n",
    "        found = torch.gather(target, 1, spred)\n",
    "        temp = torch.arange(preds.size(1)).cpu().float() * 1e-6\n",
    "        _, sel = torch.topk(found - temp, 1, dim=1)\n",
    "        \n",
    "        sel = sel.float()\n",
    "        sel[~has_positives] = torch.nan\n",
    "        \n",
    "        mr1 = torch.nanmean((sel+1).float())\n",
    "\n",
    "        del sel, found, temp, spred, has_positives\n",
    "        torch.cuda.empty_cache()\n",
    "        return mr1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_audio_preds(model, dataset):\n",
    "\n",
    "    # get audio preds\n",
    "    data = get_dataset(model, dataset)\n",
    "    preds = data.get_csi_pred_matrix(model).cpu()\n",
    "    preds = torch.where(preds == float('-inf'), 0, preds)\n",
    "    return preds\n",
    "\n",
    "def get_fuzzy_preds(dataset):\n",
    "\n",
    "    # get text preds\n",
    "    blocker = Blocker(blocking_func=fuzz.token_ratio, threshold=0.5)\n",
    "    left_df, right_df = dataset.get_dfs_by_task(\"svShort\")\n",
    "    preds = blocker.predict(left_df, right_df).cpu()\n",
    "    preds = preds.fill_diagonal_(-float('inf')) / 100\n",
    "    preds = torch.where(preds == float('-inf'), 0, preds)\n",
    "    return preds\n",
    "\n",
    "\n",
    "def get_text_preds(model, dataset):\n",
    "    if model == \"fuzzy\":\n",
    "        return get_fuzzy_preds(get_dataset(model, dataset))\n",
    "    else:\n",
    "        return torch.load(f\"preds/{model}/{dataset}/preds.pt\")\n",
    "\n",
    "\n",
    "def get_model_mode(model):\n",
    "    if model == \"fuzzy\" or model == \"sentence-transformers\":\n",
    "        return \"tvShort\"\n",
    "    elif model == \"ditto\" or model == \"rsupcon\":\n",
    "        return \"rLong\"\n",
    "    elif model == \"hiergat_split\":\n",
    "        return \"rShort\"\n",
    "\n",
    "\n",
    "def get_dataset(model, dataset):\n",
    "    csi_path = \"/data/csi_datasets/\"\n",
    "    metadata_path = \"/data/yt_metadata.parquet\"\n",
    "    if model == \"sentence-transformers\":\n",
    "        return OnlineCoverSongDataset(\n",
    "                dataset,\n",
    "                csi_path,\n",
    "                metadata_path,\n",
    "                get_model_mode(model)\n",
    "        )  \n",
    "    else:\n",
    "        return TestDataset(\n",
    "        dataset,\n",
    "        csi_path,\n",
    "        metadata_path,\n",
    "        tokenizer=\"roberta-base\"\n",
    "        )\n",
    "\n",
    "\n",
    "def get_ensemble_data(text_model, audio_model, dataset):\n",
    "    \n",
    "    data = get_dataset(text_model, dataset)\n",
    "    \n",
    "    # get preds\n",
    "    text_preds = get_text_preds(text_model, dataset).cpu().numpy()\n",
    "    audio_preds = get_audio_preds(audio_model, dataset).cpu().numpy()\n",
    "\n",
    "    # get ground truth\n",
    "    Y = data.get_target_matrix().to(float).cpu()\n",
    "    \n",
    "    # get indexes\n",
    "    m, n = Y.shape\n",
    "    indexes = torch.arange(m).view(-1, 1).expand(-1, n).cpu()\n",
    "\n",
    "    # last transform\n",
    "    y_train = Y.cpu().numpy().flatten()\n",
    "    X_train = np.concatenate([text_preds.reshape(-1, 1), audio_preds.reshape(-1, 1)], axis=1)\n",
    "\n",
    "    # get query info array\n",
    "    qids = indexes.cpu().numpy().flatten()\n",
    "    return X_train, y_train, qids\n",
    "\n",
    "def compute_metrics(X_test, y_test, qids, ltr_model, out_path):\n",
    "\n",
    "    preds = ltr_model.predict(X_test)\n",
    "    # unflatten\n",
    "    def unflatten(t):\n",
    "        return torch.tensor(t.reshape((int(np.sqrt(len(t))), int(np.sqrt(len(t))))))\n",
    "    \n",
    "    preds = unflatten(preds)\n",
    "    # normalize\n",
    "    preds = (preds - torch.min(preds)) / (torch.max(preds) - torch.min(preds))\n",
    "    \n",
    "    target = unflatten(y_test)\n",
    "    indexes = unflatten(qids)\n",
    "\n",
    "    torch.save(preds, os.path.join(out_path, \"ypreds.pt\"))\n",
    "    torch.save(target, os.path.join(out_path, \"ytrue.pt\"))\n",
    "\n",
    "    map_result = mean_average_precision(preds.cpu(), target.cpu(), indexes.cpu())\n",
    "    mr1_result = mean_rank_1(preds, target)\n",
    "    return map_result, mr1_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"objective\": \"rank:map\", \n",
    "    \"lambdarank_pair_method\": \"topk\", \n",
    "    \"lambdarank_num_pair_per_sample\": 50\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fuzzy Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-8 {color: black;}#sk-container-id-8 pre{padding: 0;}#sk-container-id-8 div.sk-toggleable {background-color: white;}#sk-container-id-8 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-8 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-8 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-8 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-8 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-8 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-8 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-8 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-8 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-8 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-8 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-8 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-8 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-8 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-8 div.sk-item {position: relative;z-index: 1;}#sk-container-id-8 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-8 div.sk-item::before, #sk-container-id-8 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-8 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-8 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-8 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-8 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-8 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-8 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-8 div.sk-label-container {text-align: center;}#sk-container-id-8 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-8 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-8\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBRanker(base_score=None, booster=None, callbacks=None, colsample_bylevel=None,\n",
       "          colsample_bynode=None, colsample_bytree=None, device=None,\n",
       "          early_stopping_rounds=None, enable_categorical=False,\n",
       "          eval_metric=None, feature_types=None, gamma=None, grow_policy=None,\n",
       "          importance_type=None, interaction_constraints=None,\n",
       "          lambdarank_num_pair_per_sample=50, lambdarank_pair_method=&#x27;topk&#x27;,\n",
       "          learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
       "          max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
       "          max_leaves=None, min_child_weight=None, missing=nan,\n",
       "          monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
       "          n_jobs=None, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" checked><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBRanker</label><div class=\"sk-toggleable__content\"><pre>XGBRanker(base_score=None, booster=None, callbacks=None, colsample_bylevel=None,\n",
       "          colsample_bynode=None, colsample_bytree=None, device=None,\n",
       "          early_stopping_rounds=None, enable_categorical=False,\n",
       "          eval_metric=None, feature_types=None, gamma=None, grow_policy=None,\n",
       "          importance_type=None, interaction_constraints=None,\n",
       "          lambdarank_num_pair_per_sample=50, lambdarank_pair_method=&#x27;topk&#x27;,\n",
       "          learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
       "          max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
       "          max_leaves=None, min_child_weight=None, missing=nan,\n",
       "          monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
       "          n_jobs=None, ...)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "XGBRanker(base_score=None, booster=None, callbacks=None, colsample_bylevel=None,\n",
       "          colsample_bynode=None, colsample_bytree=None, device=None,\n",
       "          early_stopping_rounds=None, enable_categorical=False,\n",
       "          eval_metric=None, feature_types=None, gamma=None, grow_policy=None,\n",
       "          importance_type=None, interaction_constraints=None,\n",
       "          lambdarank_num_pair_per_sample=50, lambdarank_pair_method='topk',\n",
       "          learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
       "          max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
       "          max_leaves=None, min_child_weight=None, missing=nan,\n",
       "          monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
       "          n_jobs=None, ...)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "X_train, y_train, qids_train = get_ensemble_data(\"fuzzy\", \"coverhunter\", \"shs100k_1000\")\n",
    "X_val, y_val, qids_val = get_ensemble_data(\"fuzzy\", \"coverhunter\", \"shs100k2_val\")\n",
    "\n",
    "model_fuzzy_ch = xgb.XGBRanker(**params)\n",
    "model_fuzzy_ch.fit(X_train, y_train, qid=qids_train, eval_set=[(X_val, y_val)], eval_qid=[qids_val])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.8973), tensor(4.3458))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_model = \"fuzzy\"\n",
    "audio_model = \"coverhunter\"\n",
    "dataset = \"shs100k2_test\"\n",
    "out_path = os.path.join(\"preds\", f\"{text_model}_{audio_model}\", dataset)\n",
    "os.makedirs(out_path, exist_ok=True)\n",
    "\n",
    "X_test, y_test, qids_test = get_ensemble_data(text_model, audio_model , dataset)\n",
    "mapr, mr1r = compute_metrics(X_test, y_test, qids_test, model_fuzzy_ch, out_path)\n",
    "mapr, mr1r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.8469), tensor(4.7966))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_model = \"fuzzy\"\n",
    "audio_model = \"coverhunter\"\n",
    "dataset = \"da-tacos\"\n",
    "out_path = os.path.join(\"preds\", f\"{text_model}_{audio_model}\", dataset)\n",
    "os.makedirs(out_path, exist_ok=True)\n",
    "\n",
    "X_test, y_test, qids_test = get_ensemble_data(text_model, audio_model , dataset)\n",
    "mapr, mr1r = compute_metrics(X_test, y_test, qids_test, model_fuzzy_ch, out_path)\n",
    "mapr, mr1r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-9 {color: black;}#sk-container-id-9 pre{padding: 0;}#sk-container-id-9 div.sk-toggleable {background-color: white;}#sk-container-id-9 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-9 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-9 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-9 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-9 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-9 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-9 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-9 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-9 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-9 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-9 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-9 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-9 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-9 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-9 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-9 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-9 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-9 div.sk-item {position: relative;z-index: 1;}#sk-container-id-9 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-9 div.sk-item::before, #sk-container-id-9 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-9 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-9 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-9 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-9 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-9 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-9 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-9 div.sk-label-container {text-align: center;}#sk-container-id-9 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-9 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-9\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBRanker(base_score=None, booster=None, callbacks=None, colsample_bylevel=None,\n",
       "          colsample_bynode=None, colsample_bytree=None, device=None,\n",
       "          early_stopping_rounds=None, enable_categorical=False,\n",
       "          eval_metric=None, feature_types=None, gamma=None, grow_policy=None,\n",
       "          importance_type=None, interaction_constraints=None,\n",
       "          lambdarank_num_pair_per_sample=50, lambdarank_pair_method=&#x27;topk&#x27;,\n",
       "          learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
       "          max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
       "          max_leaves=None, min_child_weight=None, missing=nan,\n",
       "          monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
       "          n_jobs=None, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-9\" type=\"checkbox\" checked><label for=\"sk-estimator-id-9\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBRanker</label><div class=\"sk-toggleable__content\"><pre>XGBRanker(base_score=None, booster=None, callbacks=None, colsample_bylevel=None,\n",
       "          colsample_bynode=None, colsample_bytree=None, device=None,\n",
       "          early_stopping_rounds=None, enable_categorical=False,\n",
       "          eval_metric=None, feature_types=None, gamma=None, grow_policy=None,\n",
       "          importance_type=None, interaction_constraints=None,\n",
       "          lambdarank_num_pair_per_sample=50, lambdarank_pair_method=&#x27;topk&#x27;,\n",
       "          learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
       "          max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
       "          max_leaves=None, min_child_weight=None, missing=nan,\n",
       "          monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
       "          n_jobs=None, ...)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "XGBRanker(base_score=None, booster=None, callbacks=None, colsample_bylevel=None,\n",
       "          colsample_bynode=None, colsample_bytree=None, device=None,\n",
       "          early_stopping_rounds=None, enable_categorical=False,\n",
       "          eval_metric=None, feature_types=None, gamma=None, grow_policy=None,\n",
       "          importance_type=None, interaction_constraints=None,\n",
       "          lambdarank_num_pair_per_sample=50, lambdarank_pair_method='topk',\n",
       "          learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
       "          max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
       "          max_leaves=None, min_child_weight=None, missing=nan,\n",
       "          monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
       "          n_jobs=None, ...)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, y_train, qids_train = get_ensemble_data(\"fuzzy\", \"cqtnet\", \"shs100k_1000\")\n",
    "X_val, y_val, qids_val = get_ensemble_data(\"fuzzy\", \"cqtnet\", \"shs100k2_val\")\n",
    "\n",
    "model_fuzzy_cq= xgb.XGBRanker(**params)\n",
    "model_fuzzy_cq.fit(X_train, y_train, qid=qids_train, eval_set=[(X_val, y_val)], eval_qid=[qids_val])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.7523), tensor(15.4328))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_model = \"fuzzy\"\n",
    "audio_model = \"cqtnet\"\n",
    "dataset = \"shs100k2_test\"\n",
    "out_path = os.path.join(\"preds\", f\"{text_model}_{audio_model}\", dataset)\n",
    "os.makedirs(out_path, exist_ok=True)\n",
    "\n",
    "X_test, y_test, qids_test = get_ensemble_data(text_model, audio_model , dataset)\n",
    "mapr, mr1r = compute_metrics(X_test, y_test, qids_test, model_fuzzy_cq, out_path)\n",
    "mapr, mr1r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.8050), tensor(4.0061))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_model = \"fuzzy\"\n",
    "audio_model = \"cqtnet\"\n",
    "dataset = \"da-tacos\"\n",
    "out_path = os.path.join(\"preds\", f\"{text_model}_{audio_model}\", dataset)\n",
    "os.makedirs(out_path, exist_ok=True)\n",
    "\n",
    "X_test, y_test, qids_test = get_ensemble_data(text_model, audio_model , dataset)\n",
    "mapr, mr1r = compute_metrics(X_test, y_test, qids_test, model_fuzzy_cq, out_path)\n",
    "mapr, mr1r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# S-BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-10 {color: black;}#sk-container-id-10 pre{padding: 0;}#sk-container-id-10 div.sk-toggleable {background-color: white;}#sk-container-id-10 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-10 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-10 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-10 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-10 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-10 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-10 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-10 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-10 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-10 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-10 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-10 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-10 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-10 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-10 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-10 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-10 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-10 div.sk-item {position: relative;z-index: 1;}#sk-container-id-10 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-10 div.sk-item::before, #sk-container-id-10 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-10 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-10 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-10 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-10 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-10 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-10 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-10 div.sk-label-container {text-align: center;}#sk-container-id-10 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-10 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-10\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBRanker(base_score=None, booster=None, callbacks=None, colsample_bylevel=None,\n",
       "          colsample_bynode=None, colsample_bytree=None, device=None,\n",
       "          early_stopping_rounds=None, enable_categorical=False,\n",
       "          eval_metric=None, feature_types=None, gamma=None, grow_policy=None,\n",
       "          importance_type=None, interaction_constraints=None,\n",
       "          lambdarank_num_pair_per_sample=50, lambdarank_pair_method=&#x27;topk&#x27;,\n",
       "          learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
       "          max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
       "          max_leaves=None, min_child_weight=None, missing=nan,\n",
       "          monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
       "          n_jobs=None, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-10\" type=\"checkbox\" checked><label for=\"sk-estimator-id-10\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBRanker</label><div class=\"sk-toggleable__content\"><pre>XGBRanker(base_score=None, booster=None, callbacks=None, colsample_bylevel=None,\n",
       "          colsample_bynode=None, colsample_bytree=None, device=None,\n",
       "          early_stopping_rounds=None, enable_categorical=False,\n",
       "          eval_metric=None, feature_types=None, gamma=None, grow_policy=None,\n",
       "          importance_type=None, interaction_constraints=None,\n",
       "          lambdarank_num_pair_per_sample=50, lambdarank_pair_method=&#x27;topk&#x27;,\n",
       "          learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
       "          max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
       "          max_leaves=None, min_child_weight=None, missing=nan,\n",
       "          monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
       "          n_jobs=None, ...)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "XGBRanker(base_score=None, booster=None, callbacks=None, colsample_bylevel=None,\n",
       "          colsample_bynode=None, colsample_bytree=None, device=None,\n",
       "          early_stopping_rounds=None, enable_categorical=False,\n",
       "          eval_metric=None, feature_types=None, gamma=None, grow_policy=None,\n",
       "          importance_type=None, interaction_constraints=None,\n",
       "          lambdarank_num_pair_per_sample=50, lambdarank_pair_method='topk',\n",
       "          learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
       "          max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
       "          max_leaves=None, min_child_weight=None, missing=nan,\n",
       "          monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
       "          n_jobs=None, ...)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, y_train, qids_train = get_ensemble_data(\"sentence-transformers\", \"coverhunter\", \"shs100k_1000\")\n",
    "X_val, y_val, qids_val = get_ensemble_data(\"sentence-transformers\", \"coverhunter\", \"shs100k2_val\")\n",
    "\n",
    "model_sbert_ch = xgb.XGBRanker(**params)\n",
    "model_sbert_ch.fit(X_train, y_train, qid=qids_train, eval_set=[(X_val, y_val)], eval_qid=[qids_val])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'preds/sentence-transformers/shs100k2_test/preds.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m out_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpreds\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtext_model\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maudio_model\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, dataset)\n\u001b[1;32m      5\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(out_path, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m----> 7\u001b[0m X_test, y_test, qids_test \u001b[38;5;241m=\u001b[39m \u001b[43mget_ensemble_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio_model\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m mapr, mr1r \u001b[38;5;241m=\u001b[39m compute_metrics(X_test, y_test, qids_test, model_sbert_ch, out_path)\n\u001b[1;32m      9\u001b[0m mapr, mr1r\n",
      "Cell \u001b[0;32mIn[38], line 60\u001b[0m, in \u001b[0;36mget_ensemble_data\u001b[0;34m(text_model, audio_model, dataset)\u001b[0m\n\u001b[1;32m     57\u001b[0m data \u001b[38;5;241m=\u001b[39m get_dataset(text_model, dataset)\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m# get preds\u001b[39;00m\n\u001b[0;32m---> 60\u001b[0m text_preds \u001b[38;5;241m=\u001b[39m \u001b[43mget_text_preds\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m     61\u001b[0m audio_preds \u001b[38;5;241m=\u001b[39m get_audio_preds(audio_model, dataset)\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# get ground truth\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[38], line 24\u001b[0m, in \u001b[0;36mget_text_preds\u001b[0;34m(model, dataset)\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m get_fuzzy_preds(get_dataset(model, dataset))\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 24\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpreds/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mmodel\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mdataset\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/preds.pt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/miniconda3/envs/torch21/lib/python3.11/site-packages/torch/serialization.py:986\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m    983\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    984\u001b[0m     pickle_load_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 986\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[1;32m    987\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m    988\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m    989\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m    990\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m    991\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[0;32m/data/miniconda3/envs/torch21/lib/python3.11/site-packages/torch/serialization.py:435\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    433\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    434\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 435\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    436\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    437\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[0;32m/data/miniconda3/envs/torch21/lib/python3.11/site-packages/torch/serialization.py:416\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    415\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[0;32m--> 416\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mopen\u001b[39m(name, mode))\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'preds/sentence-transformers/shs100k2_test/preds.pt'"
     ]
    }
   ],
   "source": [
    "text_model = \"sentence-transformers\"\n",
    "audio_model = \"coverhunter\"\n",
    "dataset = \"shs100k2_test\"\n",
    "out_path = os.path.join(\"preds\", f\"{text_model}_{audio_model}\", dataset)\n",
    "os.makedirs(out_path, exist_ok=True)\n",
    "\n",
    "X_test, y_test, qids_test = get_ensemble_data(text_model, audio_model , dataset)\n",
    "mapr, mr1r = compute_metrics(X_test, y_test, qids_test, model_sbert_ch, out_path)\n",
    "mapr, mr1r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.9340), tensor(3.0018))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_model = \"sentence-transformers\"\n",
    "audio_model = \"coverhunter\"\n",
    "dataset = \"da-tacos\"\n",
    "out_path = os.path.join(\"preds\", f\"{text_model}_{audio_model}\", dataset)\n",
    "os.makedirs(out_path, exist_ok=True)\n",
    "\n",
    "X_test, y_test, qids_test = get_ensemble_data(text_model, audio_model , dataset)\n",
    "mapr, mr1r = compute_metrics(X_test, y_test, qids_test, model_sbert_ch, out_path)\n",
    "mapr, mr1r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {color: black;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBRanker(base_score=None, booster=None, callbacks=None, colsample_bylevel=None,\n",
       "          colsample_bynode=None, colsample_bytree=None, device=None,\n",
       "          early_stopping_rounds=None, enable_categorical=False,\n",
       "          eval_metric=None, feature_types=None, gamma=None, grow_policy=None,\n",
       "          importance_type=None, interaction_constraints=None,\n",
       "          lambdarank_num_pair_per_sample=6, lambdarank_pair_method=&#x27;topk&#x27;,\n",
       "          learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
       "          max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
       "          max_leaves=None, min_child_weight=None, missing=nan,\n",
       "          monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
       "          n_jobs=None, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBRanker</label><div class=\"sk-toggleable__content\"><pre>XGBRanker(base_score=None, booster=None, callbacks=None, colsample_bylevel=None,\n",
       "          colsample_bynode=None, colsample_bytree=None, device=None,\n",
       "          early_stopping_rounds=None, enable_categorical=False,\n",
       "          eval_metric=None, feature_types=None, gamma=None, grow_policy=None,\n",
       "          importance_type=None, interaction_constraints=None,\n",
       "          lambdarank_num_pair_per_sample=6, lambdarank_pair_method=&#x27;topk&#x27;,\n",
       "          learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
       "          max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
       "          max_leaves=None, min_child_weight=None, missing=nan,\n",
       "          monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
       "          n_jobs=None, ...)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "XGBRanker(base_score=None, booster=None, callbacks=None, colsample_bylevel=None,\n",
       "          colsample_bynode=None, colsample_bytree=None, device=None,\n",
       "          early_stopping_rounds=None, enable_categorical=False,\n",
       "          eval_metric=None, feature_types=None, gamma=None, grow_policy=None,\n",
       "          importance_type=None, interaction_constraints=None,\n",
       "          lambdarank_num_pair_per_sample=6, lambdarank_pair_method='topk',\n",
       "          learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
       "          max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
       "          max_leaves=None, min_child_weight=None, missing=nan,\n",
       "          monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
       "          n_jobs=None, ...)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, y_train, qids_train = get_ensemble_data(\"sentence-transformers\", \"cqtnet\", \"shs100k_1000\")\n",
    "X_val, y_val, qids_val = get_ensemble_data(\"sentence-transformers\", \"cqtnet\", \"shs100k2_val\")\n",
    "\n",
    "model_sbert_cq = xgb.XGBRanker(**params)\n",
    "model_sbert_cq.fit(X_train, y_train, qid=qids_train, eval_set=[(X_val, y_val)], eval_qid=[qids_val])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.8454), tensor(12.1427))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_model = \"sentence-transformers\"\n",
    "audio_model = \"cqtnet\"\n",
    "dataset = \"shs100k2_test\"\n",
    "out_path = os.path.join(\"preds\", f\"{text_model}_{audio_model}\", dataset)\n",
    "os.makedirs(out_path, exist_ok=True)\n",
    "\n",
    "X_test, y_test, qids_test = get_ensemble_data(text_model, audio_model , dataset)\n",
    "mapr, mr1r = compute_metrics(X_test, y_test, qids_test, model_sbert_cq, out_path)\n",
    "mapr, mr1r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.9132), tensor(3.0571))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_model = \"sentence-transformers\"\n",
    "audio_model = \"cqtnet\"\n",
    "dataset = \"da-tacos\"\n",
    "out_path = os.path.join(\"preds\", f\"{text_model}_{audio_model}\", dataset)\n",
    "os.makedirs(out_path, exist_ok=True)\n",
    "\n",
    "X_test, y_test, qids_test = get_ensemble_data(text_model, audio_model , dataset)\n",
    "mapr, mr1r = compute_metrics(X_test, y_test, qids_test, model_sbert_cq, out_path)\n",
    "mapr, mr1r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ditto Roberta-Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBRanker(base_score=None, booster=None, callbacks=None, colsample_bylevel=None,\n",
       "          colsample_bynode=None, colsample_bytree=None, device=None,\n",
       "          early_stopping_rounds=None, enable_categorical=False,\n",
       "          eval_metric=None, feature_types=None, gamma=None, grow_policy=None,\n",
       "          importance_type=None, interaction_constraints=None,\n",
       "          lambdarank_num_pair_per_sample=6, lambdarank_pair_method=&#x27;topk&#x27;,\n",
       "          learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
       "          max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
       "          max_leaves=None, min_child_weight=None, missing=nan,\n",
       "          monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
       "          n_jobs=None, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBRanker</label><div class=\"sk-toggleable__content\"><pre>XGBRanker(base_score=None, booster=None, callbacks=None, colsample_bylevel=None,\n",
       "          colsample_bynode=None, colsample_bytree=None, device=None,\n",
       "          early_stopping_rounds=None, enable_categorical=False,\n",
       "          eval_metric=None, feature_types=None, gamma=None, grow_policy=None,\n",
       "          importance_type=None, interaction_constraints=None,\n",
       "          lambdarank_num_pair_per_sample=6, lambdarank_pair_method=&#x27;topk&#x27;,\n",
       "          learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
       "          max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
       "          max_leaves=None, min_child_weight=None, missing=nan,\n",
       "          monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
       "          n_jobs=None, ...)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "XGBRanker(base_score=None, booster=None, callbacks=None, colsample_bylevel=None,\n",
       "          colsample_bynode=None, colsample_bytree=None, device=None,\n",
       "          early_stopping_rounds=None, enable_categorical=False,\n",
       "          eval_metric=None, feature_types=None, gamma=None, grow_policy=None,\n",
       "          importance_type=None, interaction_constraints=None,\n",
       "          lambdarank_num_pair_per_sample=6, lambdarank_pair_method='topk',\n",
       "          learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
       "          max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
       "          max_leaves=None, min_child_weight=None, missing=nan,\n",
       "          monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
       "          n_jobs=None, ...)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#X_train, y_train, qids_train = get_ensemble_data(\"ditto/roberta-base\", \"coverhunter\", \"shs100k_1000\")\n",
    "#X_val, y_val, qids_val = get_ensemble_data(\"ditto\", \"coverhunter\", \"shs100k2_val_balanced\")\n",
    "\n",
    "#model_ditto_ch = xgb.XGBRanker(**params)\n",
    "#model_ditto_ch.fit(X_train, y_train, qid=qids_train, eval_set=[(X_val, y_val)], eval_qid=[qids_val])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.9150), tensor(6.7781))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_model = \"ditto/roberta-base\"\n",
    "audio_model = \"coverhunter\"\n",
    "dataset = \"shs100k2_test\"\n",
    "out_path = os.path.join(\"preds\", f\"{text_model}_{audio_model}\", dataset)\n",
    "os.makedirs(out_path, exist_ok=True)\n",
    "\n",
    "X_test, y_test, qids_test = get_ensemble_data(text_model, audio_model , dataset)\n",
    "mapr, mr1r = compute_metrics(X_test, y_test, qids_test, model_ditto_ch, out_path)\n",
    "mapr, mr1r\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.9299), tensor(5.4058))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_model = \"ditto/roberta-base\"\n",
    "audio_model = \"coverhunter\"\n",
    "dataset = \"shs100k2_test\"\n",
    "out_path = os.path.join(\"preds\", f\"{text_model}_{audio_model}\", dataset)\n",
    "os.makedirs(out_path, exist_ok=True)\n",
    "\n",
    "X_test, y_test, qids_test = get_ensemble_data(text_model, audio_model , dataset)\n",
    "mapr, mr1r = compute_metrics(X_test, y_test, qids_test, model_sbert_ch, out_path)\n",
    "mapr, mr1r\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.9257), tensor(2.6966))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_model = \"ditto/roberta-base\"\n",
    "audio_model = \"coverhunter\"\n",
    "dataset = \"da-tacos\"\n",
    "out_path = os.path.join(\"preds\", f\"{text_model}_{audio_model}\", dataset)\n",
    "os.makedirs(out_path, exist_ok=True)\n",
    "\n",
    "X_test, y_test, qids_test = get_ensemble_data(text_model, audio_model , dataset)\n",
    "mapr, mr1r = compute_metrics(X_test, y_test, qids_test, model_ditto_ch, out_path)\n",
    "mapr, mr1r\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.9347), tensor(2.2617))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_model = \"ditto/roberta-base\"\n",
    "audio_model = \"coverhunter\"\n",
    "dataset = \"da-tacos\"\n",
    "out_path = os.path.join(\"preds\", f\"{text_model}_{audio_model}\", dataset)\n",
    "os.makedirs(out_path, exist_ok=True)\n",
    "\n",
    "X_test, y_test, qids_test = get_ensemble_data(text_model, audio_model , dataset)\n",
    "mapr, mr1r = compute_metrics(X_test, y_test, qids_test, model_sbert_ch, out_path)\n",
    "mapr, mr1r\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-6 {color: black;}#sk-container-id-6 pre{padding: 0;}#sk-container-id-6 div.sk-toggleable {background-color: white;}#sk-container-id-6 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-6 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-6 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-6 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-6 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-6 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-6 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-6 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-6 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-6 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-6 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-6 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-6 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-6 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-6 div.sk-item {position: relative;z-index: 1;}#sk-container-id-6 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-6 div.sk-item::before, #sk-container-id-6 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-6 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-6 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-6 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-6 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-6 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-6 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-6 div.sk-label-container {text-align: center;}#sk-container-id-6 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-6 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-6\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBRanker(base_score=None, booster=None, callbacks=None, colsample_bylevel=None,\n",
       "          colsample_bynode=None, colsample_bytree=None, device=None,\n",
       "          early_stopping_rounds=None, enable_categorical=False,\n",
       "          eval_metric=None, feature_types=None, gamma=None, grow_policy=None,\n",
       "          importance_type=None, interaction_constraints=None,\n",
       "          lambdarank_num_pair_per_sample=6, lambdarank_pair_method=&#x27;topk&#x27;,\n",
       "          learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
       "          max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
       "          max_leaves=None, min_child_weight=None, missing=nan,\n",
       "          monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
       "          n_jobs=None, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" checked><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBRanker</label><div class=\"sk-toggleable__content\"><pre>XGBRanker(base_score=None, booster=None, callbacks=None, colsample_bylevel=None,\n",
       "          colsample_bynode=None, colsample_bytree=None, device=None,\n",
       "          early_stopping_rounds=None, enable_categorical=False,\n",
       "          eval_metric=None, feature_types=None, gamma=None, grow_policy=None,\n",
       "          importance_type=None, interaction_constraints=None,\n",
       "          lambdarank_num_pair_per_sample=6, lambdarank_pair_method=&#x27;topk&#x27;,\n",
       "          learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
       "          max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
       "          max_leaves=None, min_child_weight=None, missing=nan,\n",
       "          monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
       "          n_jobs=None, ...)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "XGBRanker(base_score=None, booster=None, callbacks=None, colsample_bylevel=None,\n",
       "          colsample_bynode=None, colsample_bytree=None, device=None,\n",
       "          early_stopping_rounds=None, enable_categorical=False,\n",
       "          eval_metric=None, feature_types=None, gamma=None, grow_policy=None,\n",
       "          importance_type=None, interaction_constraints=None,\n",
       "          lambdarank_num_pair_per_sample=6, lambdarank_pair_method='topk',\n",
       "          learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
       "          max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
       "          max_leaves=None, min_child_weight=None, missing=nan,\n",
       "          monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
       "          n_jobs=None, ...)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#X_train, y_train, qids_train = get_ensemble_data(\"ditto/roberta-base\", \"cqtnet\", \"shs100k_1000\")\n",
    "#X_val, y_val, qids_val = get_ensemble_data(\"ditto\", \"cqtnet\", \"shs100k2_val_balanced\")\n",
    "\n",
    "#model_ditto_cq = xgb.XGBRanker(**params)\n",
    "#model_ditto_cq.fit(X_train, y_train, qid=qids_train, eval_set=[(X_val, y_val)], eval_qid=[qids_val])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.9152), tensor(3.0624))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_model = \"ditto/roberta-base\"\n",
    "audio_model = \"cqtnet\"\n",
    "dataset = \"da-tacos\"\n",
    "out_path = os.path.join(\"preds\", f\"{text_model}_{audio_model}\", dataset)\n",
    "os.makedirs(out_path, exist_ok=True)\n",
    "\n",
    "X_test, y_test, qids_test = get_ensemble_data(text_model, audio_model , dataset)\n",
    "mapr, mr1r = compute_metrics(X_test, y_test, qids_test, model_sbert_cq, out_path)\n",
    "mapr, mr1r\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.8457), tensor(16.2877))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_model = \"ditto/roberta-base\"\n",
    "audio_model = \"cqtnet\"\n",
    "dataset = \"shs100k2_test\"\n",
    "out_path = os.path.join(\"preds\", f\"{text_model}_{audio_model}\", dataset)\n",
    "os.makedirs(out_path, exist_ok=True)\n",
    "\n",
    "X_test, y_test, qids_test = get_ensemble_data(text_model, audio_model , dataset)\n",
    "mapr, mr1r = compute_metrics(X_test, y_test, qids_test, model_sbert_cq, out_path)\n",
    "mapr, mr1r\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ditto BERT Multilingual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, qids_train = get_ensemble_data(\"ditto/bert-base-multilingual-cased\", \"coverhunter\", \"shs100k_1000\")\n",
    "X_val, y_val, qids_val = get_ensemble_data(\"ditto\", \"coverhunter\", \"shs100k2_val_balanced\")\n",
    "\n",
    "model_ditto_ch = xgb.XGBRanker(**params)\n",
    "model_ditto_ch.fit(X_train, y_train, qid=qids_train, eval_set=[(X_val, y_val)], eval_qid=[qids_val])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.9190), tensor(7.0597))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_model = \"ditto/bert-base-multilingual-cased\"\n",
    "audio_model = \"coverhunter\"\n",
    "dataset = \"shs100k2_test\"\n",
    "out_path = os.path.join(\"preds\", f\"{text_model}_{audio_model}\", dataset)\n",
    "os.makedirs(out_path, exist_ok=True)\n",
    "\n",
    "X_test, y_test, qids_test = get_ensemble_data(text_model, audio_model , dataset)\n",
    "mapr, mr1r = compute_metrics(X_test, y_test, qids_test, model_sbert_ch, out_path)\n",
    "mapr, mr1r\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.9160), tensor(2.7023))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_model = \"ditto/bert-base-multilingual-cased\"\n",
    "audio_model = \"coverhunter\"\n",
    "dataset = \"da-tacos\"\n",
    "out_path = os.path.join(\"preds\", f\"{text_model}_{audio_model}\", dataset)\n",
    "os.makedirs(out_path, exist_ok=True)\n",
    "\n",
    "X_test, y_test, qids_test = get_ensemble_data(text_model, audio_model , dataset)\n",
    "mapr, mr1r = compute_metrics(X_test, y_test, qids_test, model_sbert_ch, out_path)\n",
    "mapr, mr1r\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.8304), tensor(20.6240))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_model = \"ditto/bert-base-multilingual-cased\"\n",
    "audio_model = \"cqtnet\"\n",
    "dataset = \"shs100k2_test\"\n",
    "out_path = os.path.join(\"preds\", f\"{text_model}_{audio_model}\", dataset)\n",
    "os.makedirs(out_path, exist_ok=True)\n",
    "\n",
    "X_test, y_test, qids_test = get_ensemble_data(text_model, audio_model , dataset)\n",
    "mapr, mr1r = compute_metrics(X_test, y_test, qids_test, model_sbert_cq, out_path)\n",
    "mapr, mr1r\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.8928), tensor(3.4938))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_model = \"ditto/bert-base-multilingual-cased\"\n",
    "audio_model = \"cqtnet\"\n",
    "dataset = \"da-tacos\"\n",
    "out_path = os.path.join(\"preds\", f\"{text_model}_{audio_model}\", dataset)\n",
    "os.makedirs(out_path, exist_ok=True)\n",
    "\n",
    "X_test, y_test, qids_test = get_ensemble_data(text_model, audio_model , dataset)\n",
    "mapr, mr1r = compute_metrics(X_test, y_test, qids_test, model_sbert_cq, out_path)\n",
    "mapr, mr1r\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.8981), tensor(8.9950))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_model = \"ditto/bert-base-multilingual-cased\"\n",
    "audio_model = \"coverhunter\"\n",
    "dataset = \"shs100k2_test\"\n",
    "out_path = os.path.join(\"preds\", f\"{text_model}_{audio_model}\", dataset)\n",
    "os.makedirs(out_path, exist_ok=True)\n",
    "\n",
    "X_test, y_test, qids_test = get_ensemble_data(text_model, audio_model , dataset)\n",
    "mapr, mr1r = compute_metrics(X_test, y_test, qids_test, model_ditto_ch, out_path)\n",
    "mapr, mr1r\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_model = \"ditto/bert-base-multilingual-cased\"\n",
    "audio_model = \"coverhunter\"\n",
    "dataset = \"da-tacos\"\n",
    "out_path = os.path.join(\"preds\", f\"{text_model}_{audio_model}\", dataset)\n",
    "os.makedirs(out_path, exist_ok=True)\n",
    "\n",
    "X_test, y_test, qids_test = get_ensemble_data(text_model, audio_model , dataset)\n",
    "mapr, mr1r = compute_metrics(X_test, y_test, qids_test, model_ditto_ch, out_path)\n",
    "mapr, mr1r\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, qids_train = get_ensemble_data(\"ditto/bert-base-multilingual-cased\", \"cqtnet\", \"shs100k_1000\")\n",
    "X_val, y_val, qids_val = get_ensemble_data(\"ditto\", \"cqtnet\", \"shs100k2_val_balanced\")\n",
    "\n",
    "model_ditto_cq = xgb.XGBRanker(**params)\n",
    "model_ditto_cq.fit(X_train, y_train, qid=qids_train, eval_set=[(X_val, y_val)], eval_qid=[qids_val])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.4645) tensor(100.2565)\n"
     ]
    }
   ],
   "source": [
    "dataset_name = \"shs100k2_test_unique\"\n",
    "tokenizer=\"bert-base-multilingual-cased\"\n",
    "\n",
    "csi_path = \"/data/csi_datasets/\"\n",
    "metadata_path = \"/data/yt_metadata.parquet\"\n",
    "dataset = TestDataset(\n",
    "    dataset_name,\n",
    "    csi_path,\n",
    "    metadata_path,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "preds = torch.load(f\"preds/ditto/{tokenizer}/{dataset_name}/preds.pt\")\n",
    "target = dataset.get_target_matrix()\n",
    "\n",
    "# get indexes\n",
    "m, n = target.shape\n",
    "indexes = torch.arange(m).view(-1, 1).expand(-1, n).cpu()\n",
    "\n",
    "map=mean_average_precision(preds.cpu(), target.cpu(), indexes.cpu())\n",
    "mr1=mean_rank_1(preds.cpu(), target.cpu())\n",
    "\n",
    "print(map, mr1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'preds/ditto/roberta-base/shs100k2_test_unique/preds.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 13\u001b[0m\n\u001b[1;32m      5\u001b[0m metadata_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/data/yt_metadata.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      6\u001b[0m dataset \u001b[38;5;241m=\u001b[39m TestDataset(\n\u001b[1;32m      7\u001b[0m     dataset_name,\n\u001b[1;32m      8\u001b[0m     csi_path,\n\u001b[1;32m      9\u001b[0m     metadata_path,\n\u001b[1;32m     10\u001b[0m     tokenizer\u001b[38;5;241m=\u001b[39mtokenizer\n\u001b[1;32m     11\u001b[0m )\n\u001b[0;32m---> 13\u001b[0m preds \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpreds/ditto/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mtokenizer\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mdataset_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/preds.pt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m target \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mget_target_matrix()\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# get indexes\u001b[39;00m\n",
      "File \u001b[0;32m/data/miniconda3/envs/torch21/lib/python3.11/site-packages/torch/serialization.py:986\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m    983\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    984\u001b[0m     pickle_load_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 986\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[1;32m    987\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m    988\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m    989\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m    990\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m    991\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[0;32m/data/miniconda3/envs/torch21/lib/python3.11/site-packages/torch/serialization.py:435\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    433\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    434\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 435\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    436\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    437\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[0;32m/data/miniconda3/envs/torch21/lib/python3.11/site-packages/torch/serialization.py:416\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    415\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[0;32m--> 416\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mopen\u001b[39m(name, mode))\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'preds/ditto/roberta-base/shs100k2_test_unique/preds.pt'"
     ]
    }
   ],
   "source": [
    "dataset_name = \"shs100k2_test_unique\"\n",
    "tokenizer=\"roberta-base\"\n",
    "\n",
    "csi_path = \"/data/csi_datasets/\"\n",
    "metadata_path = \"/data/yt_metadata.parquet\"\n",
    "dataset = TestDataset(\n",
    "    dataset_name,\n",
    "    csi_path,\n",
    "    metadata_path,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "preds = torch.load(f\"preds/ditto/{tokenizer}/{dataset_name}/preds.pt\")\n",
    "target = dataset.get_target_matrix()\n",
    "\n",
    "# get indexes\n",
    "m, n = target.shape\n",
    "indexes = torch.arange(m).view(-1, 1).expand(-1, n).cpu()\n",
    "\n",
    "map=mean_average_precision(preds.cpu(), target.cpu(), indexes.cpu())\n",
    "mr1=mean_rank_1(preds.cpu(), target.cpu())\n",
    "\n",
    "print(map, mr1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3735) tensor(189.7839)\n"
     ]
    }
   ],
   "source": [
    "dataset_name = \"shs100k2_test_unique\"\n",
    "tokenizer=None\n",
    "\n",
    "csi_path = \"/data/csi_datasets/\"\n",
    "metadata_path = \"/data/yt_metadata.parquet\"\n",
    "dataset = TestDataset(\n",
    "    dataset_name,\n",
    "    csi_path,\n",
    "    metadata_path,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "preds = torch.load(f\"preds/fuzzy/{dataset_name}/preds.pt\")\n",
    "target = dataset.get_target_matrix()\n",
    "\n",
    "# get indexes\n",
    "m, n = target.shape\n",
    "indexes = torch.arange(m).view(-1, 1).expand(-1, n).cpu()\n",
    "\n",
    "map=mean_average_precision(preds.cpu(), target.cpu(), indexes.cpu())\n",
    "mr1=mean_rank_1(preds.cpu(), target.cpu())\n",
    "\n",
    "print(map, mr1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.5470) tensor(138.0435)\n"
     ]
    }
   ],
   "source": [
    "dataset_name = \"shs100k2_test_unique\"\n",
    "tokenizer=None\n",
    "\n",
    "csi_path = \"/data/csi_datasets/\"\n",
    "metadata_path = \"/data/yt_metadata.parquet\"\n",
    "dataset = OnlineCoverSongDataset(\n",
    "    dataset_name,\n",
    "    csi_path,\n",
    "    metadata_path,\n",
    "    task=\"tvShort\"\n",
    ")\n",
    "\n",
    "preds = torch.load(f\"preds/sentence-transformers/{dataset_name}/preds.pt\")\n",
    "target = dataset.get_target_matrix()\n",
    "\n",
    "# get indexes\n",
    "m, n = target.shape\n",
    "indexes = torch.arange(m).view(-1, 1).expand(-1, n).cpu()\n",
    "\n",
    "map=mean_average_precision(preds.cpu(), target.cpu(), indexes.cpu())\n",
    "mr1=mean_rank_1(preds.cpu(), target.cpu())\n",
    "\n",
    "print(map, mr1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"shs100k2_test_unique\"\n",
    "tokenizer=None\n",
    "\n",
    "csi_path = \"/data/csi_datasets/\"\n",
    "metadata_path = \"/data/yt_metadata.parquet\"\n",
    "dataset = TestDataset(\n",
    "    dataset_name,\n",
    "    csi_path,\n",
    "    metadata_path,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "preds = torch.load(f\"preds/fuzzy/{dataset_name}/preds.pt\")\n",
    "target = dataset.get_target_matrix()\n",
    "\n",
    "# get indexes\n",
    "m, n = target.shape\n",
    "indexes = torch.arange(m).view(-1, 1).expand(-1, n).cpu()\n",
    "\n",
    "map=mean_average_precision(preds.cpu(), target.cpu(), indexes.cpu())\n",
    "mr1=mean_rank_1(preds.cpu(), target.cpu())\n",
    "\n",
    "print(map, mr1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def downsample_data(df, k, N):\n",
    "    \"\"\"\n",
    "    Downsample the DataFrame to include k items per class, dropping classes if necessary.\n",
    "\n",
    "    Parameters:\n",
    "    - df: DataFrame to be downsampled.\n",
    "    - k: Number of items per class.\n",
    "    - N: Size of the downsampled dataset.\n",
    "\n",
    "    Returns:\n",
    "    - downsampled_df: Downsampled DataFrame.\n",
    "    \"\"\"\n",
    "    # Ensure k is less than or equal to N\n",
    "    if k > N:\n",
    "        raise ValueError(\"k must be less than or equal to N.\")\n",
    "\n",
    "    # Group by 'set_id' and check the size of each group\n",
    "    group_sizes = df.groupby('set_id').size().reset_index(name='size')\n",
    "\n",
    "    # Filter groups with size less than k\n",
    "    valid_groups = group_sizes[group_sizes['size'] >= k]['set_id']\n",
    "\n",
    "    # Filter the original DataFrame to include only valid groups\n",
    "    df_valid = df[df['set_id'].isin(valid_groups)]\n",
    "\n",
    "    # Define a function to downsample each group\n",
    "    def downsample_group(group):\n",
    "        return group.sample(min(k, len(group)))\n",
    "\n",
    "    # Apply the downsampling operation to each group based on 'set_id'\n",
    "    downsampled_df = df_valid.groupby('set_id', group_keys=False).apply(downsample_group)\n",
    "\n",
    "    return downsampled_df\n",
    "\n",
    "    \n",
    "val_downsampled = downsample_data(val_data, 5, 150)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch21",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
